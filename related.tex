\section{Related Work}
\todo{add more description about DS}
\todo{ZW: Starting by saying something like: Our work lies at the intersection of numerous areas: relation extraction, xx, xx. There is no work similar to ours, with respect to modeling the noise of training data for distant supervision and then use the knowledge of noise to improve relation extraction. We then organise the related work into different areas.}

\paragraph{Distant Supervision}


\paragraph{Relation Extraction} Relation extraction aims at extracting (subject \emph{s}, relation \emph{r}, object \emph{o}) triples from free text. This task is often conducted in the distant supervision paradigm \cite{mintz2009distant}, which tries to automatically build noisy training set with the guidance of knowledge base. Specifically, it considers the sentences containing both the subject $subj$ and the object $obj$ as supports for the triple ($subj$, $rel$, $obj$). Since not all sentences retrieved by distant supervision are true positive, \cite{riedel2010modeling} propose to consider the relation extraction task as a multi-instance classification problem based on the assumption that at least one of the retrieved sentences (sentence bag) are support for the triple. \cite{hoffmann2011knowledge,surdeanu2012multi} further considers the problem as a multi-instance multi-label problem and uses graphic models to solve the problem. \cite{zeng2015distant} proposes to use piece-wise convolutional neural network (PCNN) model in the multi-instance paradigm and \cite{lin2016neural} further uses attention mechanism to better distinguish true positive ones from false positive ones. These models actually only tries to identify the noisy sentences and reduce their influence. However, our method has the ability to model the noise and can further make use of the noise.

Although the multi-instance assumption can significantly reduce the noise, this assumption is not perfect. First, there are situations where all the retrieved sentences are false positive. Second, the KB is not complete, and there may exist false negative problems. \cite{takamatsu2012reducing} considers the denoising problem as a pre-processing problem, and removes potential noisy sentences by identifying bag syntactic patterns.  Another thread of work tries to alleviate the false negative problem. \cite{xu2013filling} use pseudo-relevance feedback to find possible false negative data. \cite{ritter2013modeling} and \cite{min2013distant} adds a set of latent variables to the MultiR model \cite{hoffmann2011knowledge} and the MIML model \cite{surdeanu2012multi} respectively to model the true relation before the variables representing observed relations. Our method shares similar spirit to \cite{ritter2013modeling} and \cite{min2013distant} that we all try to generate the true relation before the observed relation. The differences between our models are three-fold. First, our model is designed in the neural network framework and their models are designed in the probabilistic graphic model framework. Second, the noise modeling part in our model is fully differentiable while their parameters used to model the transition from true relation to observed relation need to be set by hand or with some heuristics. Third, our transition matrix model can model fine grained transition from true relation to observed relation (the sentence level noise is fine grained), while their methods only deals with false negative and false positive noise.

%This method has the problem that it also removes good sentences since the sentences containing bag patterns are not always noises.

\paragraph{Noise Reduction?} Our method is also closely related to the thread of work of designing denoising neural network component in the computer vision field. The noise can come from automatically constructed dataset like Tiny Images dataset \cite{torralba200880} where the images are gathered from web search engine and can also come from human labeled dataset \cite{misra2016seeing} like the MS COCO Captions dataset \cite{chen2015microsoft}. \cite{sukhbaatar2014training} propose to use a global transition matrix to transform the true label distribution to the observed label distribution and use weight decay on the transition matrix during training. \cite{reed2014training} also use a hidden layer to represent the true label distribution but try to force it to predict both the noisy label and the input. \cite{chen2015webly,xiao2015learning} first estimates the transition matrix on the clean data set and use it in the noisy data set. \cite{misra2016seeing} generates the transition matrix dynamically for each training instance.

In contrast to computer vision, the research on denoising with neural network is limited in the field of natural language processing (NLP). \cite{fang2016learning} also uses a global transition matrix to model the noise introduced by cross-lingual projection of training data in the task of POS tagging for low-resource languages and propose to train the basic model on the clean data first and add the transition matrix when using noisy data afterwards. Similar to \cite{misra2016seeing}, we also dynamically generate a transition matrix for each training data, but our datum can be a bag of instances. Furthermore, we combine curriculum learning and trace normalization for training and we also discuss the training procedure under various situations.

%\paragraph{First Order Temporal Fact Extraction} Since most methods designed for relation extraction can be used directly to first order temporal fact extraction, the number of researches on this subtask is limited. Recently, \cite{luo2016temporal} suggests that the multi-granular nature of time expressions can help us distinguish reliable data from unreliable ones, and they divides the the distant supervision data of first order temporal fact extraction into four subsets with different levels of reliability. They further propose to dynamically generate a weight for each instance in the training set, hoping that noisy data will be assigned lower weights and have less influence on the training procedure. They also try to randomly convert the observed relation in noisy data set to \emph{no\_relation} so that the model will be more conservative when using less reliable data. The main drawback of their model is that they only try to lower the influence of the noisy data. Therefore, much of the information hidden in the noisy data are not fully used. On the other hand, our method models the noise explicitly and it has the potential to find the real relations of noisy data. Therefore, our model can make better use of noisy data and the experiments confirms our assumption.

%\paragraph{Other Related Tasks}
%Higher order temporal fact extraction focuses on finding the valid time scope of a (subject \emph{s}, relation \emph{r}, object \emph{o}) triple. The commonly used datasets are introduced by the 2011 and 2013 temporal slot filling (TSF) shared tasks \cite{Ji2011Overview,dang2013task}. Distant supervision can also be used in this task \cite{artiles2011cuny,sil2014temporal}. Although our general idea can be possibly applied to this task, the fact that it takes triples as subjects further complicates the task, and it will be non-trivial to develop the final model for this task because the basic model need to be changed. We will leave this adaptation to future work.
%Another related task is event temporal relation identification. This task is introduced by TempEval \cite{verhagen2007semeval,pustejovsky2009semeval} and aims at extracting event-time and event-event temporal relations like \emph{before}, \emph{after}, and \emph{overlap}. Unlike previous tasks, this task mainly focuses on ordering events rather than extracting triples that take time expressions as objects, and this task is often conducted in fully supervised manner where the data set is human-labeled \cite{mani2006machine,yoshikawa2009jointly}. 