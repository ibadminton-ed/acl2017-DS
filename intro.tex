\section{Introduction}

Distant supervision (\DS) is rapidly emerging as a viable means for supporting various classification tasks -- from relation extraction~\cite{mintz2009distant} and sentiment classification~\cite{go2009twitter} to cross-lingual semantic
analysis~\cite{fang2016learning}.
By distilling knowledge from seed examples, \DS automatically prepare large scalse training data for these tasks.
% By automatically aligning facts from knowledge bases to text, \DS allows us to scale a small number of seeds to millions of instances.
%\orange{By distilling feasible and effective rules from prior knowledge, this technique applys these rules to automatically prepare large scalse training data for these tasks.}
% \orange{In the context of relation extraction}, \DS works by aligning  (\texttt{subject}, \texttt{relation}, \texttt{object}) triples in knowledge base
% automatically aligning facts from knowledge bases to text, this technique allows us to scale a small number of seeds to millions of instances.


%. In the
%context of relation extraction, one could start by feeding \DS with a set annotated texts where each
%sentence is mapped to a relation, i.e., a fact tuple like $<$PERSON, \texttt{born-in}, PLACE$>$; \DS can then use the knowledge
%extracted from the texts to
%generate relation contexts from unlabeled texts.

While promising, \DS does not guarantee perfect results and often introduces noise to the
generated data. In the context of relation extraction, 
\DS works by considering sentences containing both the \emph{subj} and the \emph{obj} of a $<$\emph{subj}, \texttt{rel}, \emph{obj}$>$ triple as its supports. However, the generated data are not perfect, 
\DS could match the knowledge base (\KB) triple, $<$\emph{Donald Trump},
\texttt{born-in}, \emph{New York}$>$  in \emph{false positive} contexts like \emph{Donald Trump worked in New York City}.
Prior works~\cite{takamatsu2012reducing,ritter2013modeling} show that \DS often mistakenly labels real positive instances as negative (\emph{false negative}) or
versa vice (\emph{false positive}), and there could also have confusions among positive labels too. These noises can
severely affect training and lead to poorly-performing models.
%This drawback greatly hinders the wide adoption of this powerful technique.

Tackling the noisy data problem of \DS is non-trivial, since there usually lacks of explicit supervision to capture the noise.
%There have been attempts to tackle the noisy data problem of \DS.
Previous works have tried to remove sentences containing unreliable syntactic patterns~\cite{takamatsu2012reducing}, design new models under the
\textit{at-least-one assumption}
that at least one of the aligned sentences supports the triple in \KB~\cite{riedel2010modeling}. They attempt to introduce new variables in probabilistic graphic models to capture certain types of noise, or aggregate predictions from multiple classifiers to reduce the influence of certain noise~\cite{hoffmann2011knowledge,surdeanu2012multi,ritter2013modeling,min2013distant}. These approaches represent a substantial leap forward towards making \DS more practical. However, these methods are either tightly couple to certain types of noise,
or have to rely on manual rules to filter noise, thus are unable to scale.
% On the other hand, recent breakthrough in neural networks~\cite{lin2016neural} show that neural network models can reduce the influence of incorrectly labeled data without explicitly characterizing the inherent noise by aggregating the multiple training instances attentively for relation classification.
% \orange{In fact, we find that the neural network architecture provides opportunities to explicitly characterize the noise underlying in the. This enables the model to infer the true label of incorrectly labeled data and thus can make use of rather than trying to ignore incorrectly labeled data.}

The recent breakthrough in neural networks~\cite{lin2016neural} provides a new way to reduce the influence of incorrectly labeled data by aggregating the multiple training instances attentively for relation classification, without explicitly characterizing the inherent noise. 
Promising results in this direction have been reported in recent work~\red{\cite{}}. However, this technique is still in its early stage and much remains to be done.


In this paper, we aim to enhance \DS noise modeling by providing the capability to explicitly characterize 
the noise on the \DS output using neural networks.  We show that while noise is inevitable, it is possible to characterize the noise pattern  in a unified framework along with its original classification objective. Our key insight is that the \DS-style training  data typically contain useful clues about the noise pattern. For example, we can infer that since some people work in their birthplaces, \DS could wrongly label a training sentence describing a working place as a \texttt{born-in} relation.
Our novel approach to noisy modeling is to use a dynamically-generated transition matrix for each training instance to (1) characterize the possibility that the \DS labeled relation is confused and (2) indicate its noise pattern.  To tackle the challenge of no direct guidance over the noise pattern, we employ a curriculum learning based training method to gradually model the noise pattern over time, and use trace regularization to control the behavior of the transition matrix during training. Our approach is flexible -- while it does not make assumptions of the data quality, the algorithm can make effectively use
of the data-quality prior knowledge to guide the learning procedure when such information is available. 

We apply our method to the relation extraction task and evaluate it under various scenarios on two benchmark datasets. Experimental results show
that our approach consistently improves the learned model performance, significantly outperforming a state-of-the-art neural network based
\DS noise modeling approach\todo{cite{}}. 

Our work offers an effective way for tackling the noisy data problem of \DS, making \DS more practical in scale. This paper makes two key contributions. It is the first to (1) use a \emph{dynamic} transition matrix structure to characterize the noise introduced by \DS, and (2) employ curriculum learning to guide the training procedure performed on \DS generated data.

%This paper makes the following specific contributions. It is the first to
%\begin{itemize}[noitemsep]
%\item  use a dynamic transition matrix to characterize the noise introduced by \DS;
%\item  employ curriculum learning to guide the training procedure on \DS generated training data;
%\item offer the flexibility to exploit the prior knowledge of data quality in \DS based training.
%\end{itemize}

