\section{Introduction}

In recent years, distant supervision (\DS) is emerging as a viable means for supporting various natural language
processing (NLP) tasks -- from relation extraction~\cite{mintz2009distant} and xx~\cite{} to cross-lingual semantic
analysis~\cite{fang2016learning}. By automatically aligning
facts from knowledge bases to text, this technique allows us to scale a small number of relations to thousands.


%. In the
%context of relation extraction, one could start by feeding \DS with a set annotated texts where each
%sentence is mapped to a relation, i.e., a fact tuple like $<$PERSON, \emph{born-in}, PLACE$>$; \DS can then use the knowledge
%extracted from the texts to
%generate relation contexts from unlabeled texts.

While promising, \DS does not guarantee perfect results and often introduces noise to the
generated data. In the context of relation extraction, \DS could match the fact tuple $<$``Donald Trump",
\emph{born-in}, ``New York"$>$  in \emph{false positive} contexts like ``\emph{Donald Trump worked in New York City}".
There many prior works~\cite{} show that \DS often mistakenly labels real positive instances as negative (\emph{false negative}) or
versa vice (\emph{false positive}), and there could have confusions between positive labels too. These noises can
significantly affect the training
procedure and lead to poorly-performing models.
%This drawback greatly hinders the wide adoption of this powerful technique.

There have been attempts to tackle the noisy data problem of \DS. These
include work on xx, xxx, and xx. These works represent a substantial leap
forward towards making \DS more practical. However, prior work is shitty
because …

In this paper, we show that while noise is inevitable, it is possible to
accurately model its pattern. Our key insight is that the noise is not
random? We propose a novel approach to xxx. We apply our method to relation
extraction. To tackle xxx, we employ curriculum learning to xxx…. As
knowledge is built up, our algorithm can xxx.

We evaluate our approach by applying it to xxx and compare it against a
state-of-the-art xx. Experimental results show that our approach can create a
higher-quality relation extraction classifier with xx% better performance
using the same set of training data.
