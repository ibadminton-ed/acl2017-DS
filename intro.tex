\section{Introduction}

In recent years, distant supervision (\DS) is emerging as a viable means for supporting various classification tasks -- from relation extraction~\cite{mintz2009distant} and sentiment classification~\cite{go2009twitter} to cross-lingual semantic
analysis~\cite{fang2016learning}. By automatically aligning
facts from knowledge bases to text, this technique allows us to scale a small number of seeds to millions of instances.


%. In the
%context of relation extraction, one could start by feeding \DS with a set annotated texts where each
%sentence is mapped to a relation, i.e., a fact tuple like $<$PERSON, \emph{born-in}, PLACE$>$; \DS can then use the knowledge
%extracted from the texts to
%generate relation contexts from unlabeled texts.

While promising, \DS does not guarantee perfect results and often introduces noise to the
generated training data. In the context of relation extraction, \DS could match the knowledge tuple $<$``Donald Trump",
\emph{born-in}, ``New York"$>$  in \emph{false positive} contexts like ``\emph{Donald Trump worked in New York City}".
Many prior \red{works~\cite{}} show that \DS often mistakenly labels real positive instances as negative (\emph{false negative}) or
versa vice (\emph{false positive}), and there could also have confusions among positive labels too. These noises can
severely affect the training
procedure and lead to poorly-performing models.
%This drawback greatly hinders the wide adoption of this powerful technique.

There have been attempts to tackle the noisy data problem of \DS. Previous works have tried to preprocess the data by removing sentences containing unreliable syntactic patterns \cite{takamatsu2012reducing}, making the \textbf{\textit{at-least-one}} assumption that at least one of the aligned sentences support the knowledge triple to reduce the influence of noise \cite{riedel2010modeling} and adding a set of variables in probabilistic graphic models to \red{represent the true relation before????? the noisy relation} labeled by distant supervision \cite{hoffmann2011knowledge,surdeanu2012multi}. These works represent a substantial leap forward towards making \DS more practical. However, these methods either do not model the noise explicitly or \red{just handle false positive or false negative noise. ---------F: This argument is weak! can we say they either rely on predefined rules or extra labeled dataset?}

In this paper, we show that while noise is inevitable, it is possible to model the pattern of \blue{different} \red{all the three} types of noise in a unified form. \red{---F: This is the first time you mentioned THREE types.} Our key insight is that the input data contain useful clues about the noise pattern. For example, if we see an input sentence describing the work place of a person, we can reasonably assume that it has some chances to be erroneously labeled by distant supervision to express relation \red{$born\_in$ (---F: can we use $live\_place$??)} .
Accordingly, we propose to dynamically generate a transition matrix for each input datum to model its noise pattern. We apply our method to the relation extraction task. To tackle the difficulty of lack of direct guidance over the noise pattern, we employ curriculum learning to guide our model to learn to model the noise gradually, and use trace regularization to control the behavior of the transition matrix during training. %As knowledge is built up, our algorithm can xxx.

We evaluate our approach by applying it to \red{xxx??} and compare it against a
\red{state-of-the-art xx}. Experimental results show that \red{our approach can create a
higher-quality relation extraction classifier with xx\% better performance
using the same set of training data. (----F: Maybe we should change another type of representation? or list our contributions here?)}

This paper makes the following specific contributions. It is the first to
\begin{itemize}
\item bla 
\item bla
\item bla
\end{itemize}

