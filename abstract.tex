\begin{abstract}
Distant supervision significantly reduces human efforts in constructing
training data for classification tasks by automatically labeling data using
knowledge learned from a small set of labelled examples. While promising,
this technique often introduces noise into the dataset, which can
significantly affect the quality of the learned model. In this paper, we take
a deep look at the application of distant supervision in relation extraction.
We show that constrained dynamic transition matrices can effectively model the noise in the training data \todo{training data used by DS or generated by DS? }. 
We propose to use curriculum learning to infer the noise patterns, and
use the noise patterns to further improve the model robustness. We apply our approach
to relation extraction at both the sentence and
the bag levels. %, with and without prior knowledge of the data quality. 
The experimental results show that our method can better handle the noise over
the state-of-the-art in distantly supervised
relation extraction and improve extraction performance in various settings.
\todo{Any quantify numbers??}

\end{abstract} 