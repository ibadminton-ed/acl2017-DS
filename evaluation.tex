\section{Evaluation}
The main purposes of our experiments are to answer the following questions: 1) whether our transition matrix can model the noise in the training data collected through DS, 2) what kind of noise our approach can deal with, and 3) whether prior knowledge of data quality can help our approach better handle the noise.  
%To explore the behavior of our transition matrix method under different settings, 
We thus experiment our transition matrix method with both sentence level and bag level extraction models in the situation with and without prior knowledge of the data quality. 
%We show that our method works in all of these settings and prior knowledge of the data quality can benefit the training of transition matrix. We also find that the sentence level models works better when we have both reliable and unreliable data, but the bag level model performs better if all the data are treated equally. Furthermore, to explore the generalization ability of our method, we also conduct experiments in two datasets.
\subsection{Datasets}
We evaluate our approach on two datasets. 
The first one, (\emph{TimeRE}), is %proposed by \cite{luo2016temporal} and 
constructed in a DS style by aligning Wikidata triples with Wikipedia text,  containing \red{XXXX} sentences with \red{XXX} types of relations  between an entity mention and a time expression.  In the DS framework,   the granularity of time expressions speaks for themselves in terms of reliability, that is, given a knowledge triple $<e, rel, t>$ and its aligned,  the more fine-grained the time expression $t$ is in the instance, the more likely the instance  supports the existence of the triple. \red{we may need a concrete example.}   We thus split the dataset  split into 3 subsets with different levels of reliability. Instances with full date expressions, i.e.,  Year-Month-Day, can be seen as the most reliable data, while those with partial date expressions, e.g., Month-Year and Year-Only, can be seen as less reliable ones.  Negative data are constructed with heuristic strategies, e.g.,  \red{XXXX}. 
%According to the granularity of time expressions, this dataset can be split into 3 subsets with different levels of reliability.  
During training, we can access  \red{XXX} negative instances and  \red{XXX} positive sentences, including \red{XXX} reliable ones, \red{XXX} and \red{XXX} less reliable ones. The test set contains \red{XXX} positive instances and \red{YYY} negative instances.
%The reliable subset  is used as basic training data, validation data and test data which contains 22,214, 2,776 and 2,771 positive sentences respectively. The two less reliable subset contains 2,094 and 53,469 positive sentences and are used as additional training data. Negative data are constructed with two heuristic strategies. 
%We use this dataset because this is a public dataset on relation extraction that has both reliable and unreliable data, which is suitable for all of our experiment settings.
\red{talk bag level numbers????? and how many relations in total}

We also experiment our bag level models on a popular entity relation extraction dataset (\emph{EntityRE}) constructed by aligning triples in Freebase with the New York Times corpus (NYT corpus) \cite{riedel2010modeling}. The dataset contains 522,611 sentences and 281,270 entity pairs for training and  172,448 sentences and 96,678 entity pairs for testing. We experiment  in this dataset to see the generalization ability of our transition matrix approach.
\red{????? and how many relations in total????? why there is sentence level evaluations????}


\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/sent_time_exp_overall.png}
\caption{Sentence Level Results on TimeRE}
\label{fig: sent_luo}
\end{center}
\end{figure}

\subsection{Experimental Setup}

\paragraph{Hyperparameters} \red{can we put those parameters in a table?? especially those shared by the two settings.}
We experiment our sentence level model on TimeRE data. We use 100-dimensional word embedding pre-trained using GloVe \cite{pennington2014glove} on Wikipedia and Gigaword, and 20-dimensional vector for distance embedding. The convolution window is 3 and the number of convolution kernels is 200. The size of the full connection layer is also 200. As for training, we use stochastic gradient descend (SGD) with batch size 20, learning rate 0.1. We also use dropout with probability 0.5 upon the sentence embedding. Each data subset is added after 15 epochs since the precious one is added. The trace regularization parameters for three subsets are $\beta_1=-0.01$, $\beta_2=0.01$ and $\beta_3=0.1$ respectively from the reliable one to the most unreliable one \blue{(the ratio of $\beta_3$ and $\beta_2$ is fixed to 10 and??? 5 during hyper-parameter tuning)}.


The parameters of the bag level model is almost the same as the sentence level model on TimeRE data, except that the learning rate is 0.01. As for the EntityRE data, The word embedding is of dimension 50 and is pre-trained on the NYT corpus using word2vec\footnote{\url{ https://code.google.com/p/word2vec/}}. The convolution window is 3 and the number of convolution kernels is 256, distance embedding size is 5, batch size is 16 and learning rate is 0.01. For all the bag level models, the linear combination parameter $\alpha$ is 1 and trace regularization parameter $\beta$ is -0.1 at the start of training. We experiment with decay rate \{0.95, 0.9, 0.8\} and decay step \{3, 5, 8\}. We find that using decay rate 0.9 and decay step 5 performs best in most situations.

\paragraph{Evaluation Metrics} 
We evaluate the relation extraction performances using precision-recall (PR) curves, which is calculated \red{........}.
 
\paragraph{Baseline Settings} 
We compare our proposed approach with two extraction settings, sentence level (\emph{sent\_}) and bag level (\emph{bag\_}). In both settings, we investigate models trained on all subsets mixed together (\emph{\_mix}), models trained on reliable data only (\emph{\_reliable}), models trained with transition matrix (\emph{\_TM}), and models trained models trained with the curriculum learning setting  (\emph{\_CL}). In the bag level experiments, we also study the effect of attention (\emph{\_att}) and average (\emph{\_avg}) aggregation methods. 

\begin{figure*}[htbp]
\centering
\subfigure[Attention Aggregation]{
\includegraphics[width=0.45\linewidth]{figures/bag_att_exp_overall.png}
\label{fig: bag_att_luo}
}
\subfigure[Average Aggregation]{
\includegraphics[width=0.45\linewidth]{figures/bag_avg_exp_overall.png}
\label{fig: bag_avg_luo}
}
\caption{Bag Level Results on TimeRE}
\label{fig: results_on_luo}
\end{figure*}

\subsection{Results on TimeRE}
\paragraph{Sentence Level Models}
The results of sentence level models on TimeRE are shown in Figure \ref{fig: sent_luo}% in the form of precision recall curves (PR curves). 
We can see that %the performance of the model trained on mixed together 
mixing all subsets together (\emph{sent\_mix}) performs the worst, and is significantly worse than using the reliable subset only (\emph{sent\_reliable}),  showing the noisy nature of the training data obtained through DS. Properly dealing with the noise is the key to help DS move forward to real applications.  
When getting help from our transition matrix during training, the model (\emph{sent\_mix\_TM}) significantly improves (\emph{sent\_mix}) and performs similarly to (\emph{sent\_reliable}) in most cases, showing the ability to reduce the effect of noisy training instances. 
% obtains the ability of modeling noise (\emph{sent\_mix\_TM}), which significantly improves the performance of the model. 
On the other hand, when training on the reliable subset first and gradually adding less reliable data, %\footnote{\emph{curr} refers to curriculum learning and \emph{d} refers to data}}), 
the curriculum learning based model  (\emph{sent\_CL}) even  outperforms \emph{sent\_reliable} significantly, indicating that the curriculum learning framework does not just reduce the effect of noise, but actually help the model learn from noisy data. 
When applying transition matrix into the curriculum learning framework using one reliable subset and one less reliable one,   our model (\emph{sent\_CL\_seg2\_TM}) improves \emph{sent\_CL} by exploring the prior knowledge about data quality and enable the transition matrix approach to control the noise in different levels.  It is not surprising that when we use more less reliable data, i.e., using all three subsets, our model (\emph{sent\_CL\_TM}) significantly outperforms all other models by a large margin\footnote{We will use all three subsets for all \emph{\_CL\_TM} settings in the rest of the experiments.}.  
%  fHowever, by mixing the two unreliable subsets together and use the curriculum of training on the reliable data first as well as the transition matrix, we can see that the prior knowledge of data quality can help the transition matrix model the noise better and further improve the model performance. Furthermore, by using the curriculum of 3 subsets (\emph{sent\_currd\_TM}) instead of 2, the performance can be further improved. Therefore, 

\paragraph{Bag Level Models}
%\paragraph{Bag Level Attention Aggregation Models}
Now we will first look at the performances of the bag level models with attention aggregation, shown in Figure \ref{fig: bag_att_luo}. %We can see that the basic bag level attention aggregation model (\emph{bag\_att\_mix}) performs good and significantly outperforms the \emph{sent\_mix\_TM} model. Recall that the bag level model is based on the \emph{at-least-one assumption} that at least one of the sentences in the sentence bag support the ($subj$, $rel$, $obj$) triple, and the \emph{sent\_mix\_TM} model do not use any assumption about the dataset. This shows that prior knowledge of the data quality plays an important role in the situation where the dataset is noisy. 
Let us compare the  model trained on the reliable subset only (\emph{bag\_att\_reliable}) with the one trained on the mixed dataset (\emph{bag\_att\_mix}) first. Different from the sentence level cases, \emph{bag\_att\_mix} outperforms \emph{bag\_att\_reliable} by a large margin, due to the fact that  \emph{bag\_att\_mix} has taken the noise within the bag into consideration through the attention aggregation mechanism, which can be seen as a denoising step within the bag. 
% has some denoising ability and can make use of the noisy data. 
This may also be the reason that when we introduce either our transition matrix approach \red{(\emph{bag\_att\_TM})}  or curriculum learning framework (\emph{bag\_att\_CL})   into the bad level model , the improvements compared to \emph{bag\_att\_mix}  are not as significant as in the sentence level. 
%However, since attention aggregation already has reasonable denoising ability, we can see that although adding noisy data gradually alone works well in sentence level model, it does not improve the attention aggregation model (\emph{bag\_att\_currd}). 
However, when we utilize our transition matrix approach to control the noise of different levels within the curriculum learning paradigm (\emph{bag\_att\_CL\_TM}), the performance gets further improvement, especially in the high precision part compared to \emph{bag\_att\_CL}.  
We also note that the bag level's  \textit{at-least-one assumption} may not always hold, and there are still false negative and false positive problems. Therefore, we can see that using our transition matrix approach with  or without curriculum learning, i.e.,  \emph{bag\_att\_TM}  and \emph{bag\_att\_CL\_TM}), both improve the performance, and \emph{bag\_att\_CL\_TM} performs slightly better.

%\paragraph{Bag Level Average Aggregation Models}
The results of the bag level models with average aggregation is shown in Figure \ref{fig: bag_avg_luo}. The relative ranking of various settings is similar to those with the attention aggregation. \red{why (\emph{bag\_avg\_reliable}) performs so much better than all others in the very high precision stage ($recall<0.05$)?????????} One of the notable differences is that both \emph{bag\_avg\_CL} and \emph{bag\_avg\_TM} improve \emph{bag\_avg\_mix} with larger margins compared to that in the attention aggregation setting. The reasons may be that the average aggregation mechanism is not as good as the attention aggregation one in terms of denoising ability, which leaves more space for our transition matrix approach or curriculum learning framework to improve.   
\blue{Another prominent difference lies in the performance drop of \emph{bag\_avg\_CL\_TM} in the low-recall area ($recall<0.15$)., why???}  
%However, since  denoising ability is not as good as attention aggregation, adding unreliable data gradually (\emph{bag\_avg\_currd}) improves the model performance here. We can also see that the transition matrix improves the average aggregation models more significantly than the attention aggregation models. 
 %Note that due to the inferior denoising ability of average aggregation, the unhandled sentence level noise may further propagates to bag level, which gives the transition matrix more chance to help model the noise.
 
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/best_cmp_exp_overall.png}
\caption{Comparison on TimeRE}
\label{fig: cmp_luo}
\end{center}
\end{figure}
 
\paragraph{Comparison} \red{should change to something else??? e.g., case study, or static TM }
The comparison of the best settings of each model family is shown in Figure \ref{fig: cmp_luo}. We can see that all of our transition matrix models outperform the model of \cite{luo2016temporal}. With the help of transition matrix, although the basic version of average aggregation is not as good as attention aggregation, its transition matrix version is similar to the attention aggregation. Also note that although the sentence level models trained on mixed data do not perform very good, the sentence level model can use transition matrix to model the sentence level noise and thus performs best in all these models. Recall that the transition matrix can model the noise rather than just reduce the influence of noisy sentences as in bag level models, the sentence level model actually has the ability to make use of the noisy data. This shows that sentence level noise is more significant than the bag level noise in relation extraction, and modeling noise works better than just trying to reducing the influence of noise.


\begin{figure}[htbp]
\includegraphics[width=0.9\linewidth]{figures/re_att_avg_cmp_exp.png}
\caption{Results on Dataset of Riedel et.al.} 
\label{fig: Riedel_res}
\end{figure}

\subsection{Performance on EntityRE}
We also conduct experiments on the EntityRE dataset, where we can evaluate our bag level models only.  
%To show the generalization ability of our proposed transition matrix method,  proposed by \cite{riedel2010modeling}.
We implement the average aggregation method (\emph{avg}) and the attention aggregation method (\emph{att}) proposed by \cite{lin2016neural} as well as their corresponding transition matrix versions (\emph{avg\_TM} and \emph{att\_TM}). As we can see in Figure \ref{fig: Riedel_res},  due to the inferior denoising ability of the average aggregation component, \emph{avg} performs worst among all those models. %Similar to the trends on the TimeRE data, when 
\red{what should we say about this figure????}
When we inject our transition matrix approach into both \emph{avg} and \emph{att}, the resulting two  models, both of which can  clearly outperform their basic extraction models.  
%
%since the unhandled sentence level noise propagates to the bag level, which makes the bag level noise become more severe, the transition matrix has more chance to model the noise. Therefore, the \emph{avg\_TM} model clearly outperforms the \emph{avg} model. 
Again, because the attention aggregation model , this model already has good ability in reducing the impact of sentence level noise. Since the bag level noise is less significant than the sentence level noise, the improvement of our transition matrix model is limited, which only improves the model on the low recall part. 
%Note that the low recall part corresponds to high precision, which is more useful than the rest of the extraction results in practice. Therefore, our transition matrix method is also useful in this situation.

\iffalse
\begin{figure*}[htbp]
\centering
\subfigure[Overall PR Curves]{
\includegraphics[width=0.475\linewidth]{reg_exp_overall.png}
\label{fig: reg_overall_pr_curve}
}
\subfigure[Small Relation PR Curves]{
\includegraphics[width=0.475\linewidth]{reg_exp_small.png}
\label{fig: reg_small_rel_pr_curve}
}
\caption{Imapct of Regularization Weights} 
\label{fig: reg_PR_curve}
\end{figure*}
\fi
